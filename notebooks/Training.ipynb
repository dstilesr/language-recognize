{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import date\n",
    "from tensorflow import keras\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "dotenv.load_dotenv(os.path.join(\"..\", \".env\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and utilities\n",
    "tok = tf_text.UnicodeCharTokenizer()\n",
    "PAD_LENGTH: int = 128  # Length to which to pad sequences.\n",
    "BATCH_SIZE: int = 512  # Size of dataset batches\n",
    "VOCAB_SIZE: int = 256  # Vocabulary size (we use ASCII characters)\n",
    "DATASET_PATH: str = os.path.join(\"..\", \"data\", \"processed-text\")  # Path to dataset files\n",
    "\n",
    "# Labels\n",
    "LANGUAGES = [\n",
    "    \"English\",\n",
    "    \"French\",\n",
    "    \"Italian\",\n",
    "    \"Portuguese\",\n",
    "    \"Spanish\",\n",
    "    \"Turkish\"\n",
    "]\n",
    "\n",
    "LANGUAGES.sort()\n",
    "NUM_CLASSES: int = len(LANGUAGES)  # Number of categories to classify\n",
    "\n",
    "# Functions to prepare input for model\n",
    "def drop_newlines(strings):\n",
    "    return tf.strings.regex_replace(strings, r\"\\n\", \"\")\n",
    "\n",
    "def prepare_input(strings, labels):\n",
    "    # Remove newline characters from strings\n",
    "    strings_clean = drop_newlines(strings)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tok.tokenize(strings_clean)\n",
    "    \n",
    "    # Pad sequences\n",
    "    return tokens.to_tensor(0, shape=(tokens.shape[0], PAD_LENGTH)), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation datasets\n",
    "train_dataset = keras.preprocessing.text_dataset_from_directory(\n",
    "    os.path.join(DATASET_PATH, \"train\"),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "valid_dataset = keras.preprocessing.text_dataset_from_directory(\n",
    "    os.path.join(DATASET_PATH, \"test\"),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Preprocess data\n",
    "train_dataset = train_dataset.map(prepare_input).prefetch(2)\n",
    "valid_dataset = valid_dataset.map(prepare_input).prefetch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(embedding_dim: int = 32, lstm_dim: int = 32) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Make the prediction model!\n",
    "    \"\"\"\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    # Input tokens and embed\n",
    "    inputs = keras.layers.Input(shape=(PAD_LENGTH,), dtype=tf.int32)\n",
    "    embed = keras.layers.Embedding(\n",
    "        input_dim=VOCAB_SIZE, \n",
    "        output_dim=embedding_dim,\n",
    "        mask_zero=True,\n",
    "        input_length=PAD_LENGTH)(inputs)\n",
    "    \n",
    "    # Recurring / Convolutional layers\n",
    "    x = keras.layers.Bidirectional(keras.layers.LSTM(\n",
    "        lstm_dim,\n",
    "        activation=\"tanh\",\n",
    "        return_sequences=True))(embed)\n",
    "    \n",
    "    x = keras.layers.Conv1D(\n",
    "        2 * lstm_dim,\n",
    "        3,\n",
    "        padding=\"same\",\n",
    "        activation=\"tanh\")(x)\n",
    "    \n",
    "    # Pooling, final output\n",
    "    x = keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = keras.layers.Dense(16, activation=keras.layers.LeakyReLU())(x)\n",
    "    out = keras.layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "    \n",
    "    # Model\n",
    "    model = keras.Model(inputs=inputs, outputs=out)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(lstm_dim=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT Params\n",
    "EPOCHS = 1\n",
    "VERBOSE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(0.02),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=\"accuracy\"\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=EPOCHS, \n",
    "    validation_data=valid_dataset,\n",
    "    verbose=VERBOSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark\n",
    "test_sentences = [\n",
    "    \"Mary had a little lamb, its fleece was white as snow\",\n",
    "    \"nel mezzo del cammin di nostra vita mi ritrovai per una selva oscura\",\n",
    "    \"No tengo penas ni tengo amores y asi no sufro de sinsabores\"\n",
    "]\n",
    "\n",
    "prepared, _ = prepare_input(test_sentences, None)\n",
    "pred = model.predict(prepared)\n",
    "langs = np.argmax(pred, axis=1).tolist()\n",
    "conf = np.max(pred, axis=1).tolist()\n",
    "\n",
    "for sentence, lang, confidence in zip(test_sentences, langs, conf):\n",
    "    print(\"Sentence:\")\n",
    "    print(sentence)\n",
    "    print()\n",
    "    \n",
    "    print(\"Predicted Language:\")\n",
    "    print(LANGUAGES[lang])\n",
    "    print(\"Confidence: %.2f\" % (100 * confidence))\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and upload to S3\n",
    "model_dir = os.path.join(\"..\", \"data\", \"models\")\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "model_file = os.path.join(model_dir, \"model.h5\")\n",
    "model.save(model_file)\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.upload_file(\n",
    "    model_file,\n",
    "    os.getenv(\"S3_BUCKET\"),\n",
    "    f\"models/model-{date.today():%Y-%m-%d}.h5\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
