{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration <a id=\"title\"></a>\n",
    "Here we perform some basic exploration of the wikibooks dataset in order to get basic insights and get some ideas on how to\n",
    "process the data for training the model.\n",
    "\n",
    "## Contents\n",
    "- [Wikibooks Dataset](#wikibooks-dataset)\n",
    "  - [Data Loading](#data-loading)\n",
    "  - [Text Exploration](#text-exploration)\n",
    "- [TED Dataset](#ted-dataset)\n",
    "  - [Data Loading](#data-loading-ted)\n",
    "  - [Exploration](#exploration-ted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"seaborn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikibooks Dataset <a id=\"wikibooks-dataset\"></a>\n",
    "\n",
    "### Data Loading <a id=\"data-loading\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_file = os.path.join(\"..\", \"data\", \"raw-data\", \"french-wikibooks\", \"fr-books-dataset.csv\")\n",
    "df_fr = pd.read_csv(fr_file)\n",
    "\n",
    "print(df_fr.shape)\n",
    "print(df_fr.columns)\n",
    "\n",
    "df_fr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(df_fr).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are very few book entries with no body or abstract, so we can safely discard them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr.dropna(inplace=True)\n",
    "pd.isnull(df_fr).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr[\"body_length\"] = df_fr.body_text.map(len)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "lens_log10 = df_fr[\"body_length\"].map(np.log10)\n",
    "lens_log10.plot.hist(bins=40, ax=ax)\n",
    "\n",
    "median_len = lens_log10.median()\n",
    "ax.axvline(median_len, label=f\"Median Book Length: {10 ** median_len:0.2f}\", color=\"black\")\n",
    "\n",
    "ax.set_title(\"Hitorgram of Book Body Lengths (Characters)\")\n",
    "ax.set_xlabel(\"Length of Body (Log10)\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fr[\"abstract_length\"] = df_fr.abstract.map(len)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "lens_log10 = df_fr[\"abstract_length\"].map(np.log10)\n",
    "lens_log10.plot.hist(bins=40, ax=ax)\n",
    "\n",
    "median_len = lens_log10.median()\n",
    "ax.axvline(median_len, label=f\"Median Abstract Length (Log10): {median_len:0.2f}\", color=\"black\")\n",
    "\n",
    "ax.set_title(\"Hitorgram of Book Abstract Lengths (Characters)\")\n",
    "ax.set_xlabel(\"Length of Abstract (Log10)\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these basic histograms of body and abstract texts, we can see that mosk book texts are rather short, with a median body length of around\n",
    "5000 characters (10 ^ 3.71). This in turn corresponds to around 1000 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Exploration <a id=\"text-exploration\"></a>\n",
    "\n",
    "Now we take a quick look at one of the book bodies to get a better idea of what we are dealing with.\n",
    "\n",
    "[Back to top.](#title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(987)\n",
    "sample_df = df_fr.sample(6).reset_index(drop=True)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_df.loc[1, \"body_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_df.loc[2, \"body_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this first look we can see that there is a great variety of books within the dataset: from recipes to programming texts, with perhaps\n",
    "a predominance of the latter. This might prove challenging, since code uses mostly english keywords and as such might not be a good sample\n",
    "of the language. We could look for ways to filter out these texts, or just see if the model can perform well even if we include them. We might \n",
    "also have to deal with the diacritics present.\n",
    "\n",
    "[Back to top.](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TED Dataset <a id=\"ted-dataset\"></a>\n",
    "\n",
    "Now we explore the TED talk transcription dataset.\n",
    "\n",
    "[Back to top.](#title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading <a id=\"data-loading-ted\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted_file = os.path.join(\"..\", \"data\", \"raw-data\", \"ted-dataset\", \"ted_talks_fr.csv\")\n",
    "df_ted = pd.read_csv(ted_file)\n",
    "\n",
    "# Parse topics lists\n",
    "df_ted[\"topics\"] = df_ted[\"topics\"]\\\n",
    "    .str\\\n",
    "    .findall(r\"(?<=')\\w?[\\w\\s]+(?=')\")\n",
    "\n",
    "print(df_ted.shape)\n",
    "\n",
    "df_ted.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.isnull(df_ted).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this first view, we can see that the main field we are interested in (the transcripts of the talk) has no null values. Now we\n",
    "move on to explore properties of the dataset as well as view some texts.\n",
    "\n",
    "### Exploration <a id=\"exploration-ted\"></a>\n",
    "\n",
    "[Back to top.](#title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ted[\"talk_length\"] = df_ted[\"transcript\"]\\\n",
    "    .map(len)\\\n",
    "    .map(np.log10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "median_len = df_ted[\"talk_length\"].median()\n",
    "df_ted[\"talk_length\"].plot.hist(ax=ax, bins=35)\n",
    "\n",
    "ax.axvline(median_len, color=\"black\", label=f\"Median Length: {10 ** median_len:.2f}\")\n",
    "ax.legend()\n",
    "\n",
    "ax.set_title(\"Length of Transcript (Chars) Histogram (Log10)\")\n",
    "ax.set_xlabel(\"log10(talk length)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics = []\n",
    "for i in df_ted.index:\n",
    "    all_topics.extend(df_ted.loc[i, \"topics\"])\n",
    "\n",
    "all_topics = pd.Series(all_topics)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "all_topics.value_counts()[:25]\\\n",
    "    .plot.bar(ax=ax)\n",
    "ax.set_title(\"Most Common Topics in Talks\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_xlabel(\"Topic\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that the median length of the transcripts of TED talks is twice as long as the book bodies from the Wikibooks\n",
    "dataset. It is also to be expected that the talks will have fewer sections of code or other noise and as such may be a more useful\n",
    "dataset for our purposes. Now we look at some transcript samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(854)\n",
    "sample_ted = df_ted.sample(6)[[\n",
    "    \"talk_id\",\n",
    "    \"title\",\n",
    "    \"all_speakers\",\n",
    "    \"description\",\n",
    "    \"transcript\"\n",
    "]].reset_index(drop=True)\n",
    "\n",
    "sample_ted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_ted.loc[2, \"transcript\"][:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a first glance, this looks like a much more convenient dataset for our model, since there are unlikely to be any code sections\n",
    "or noise, and we just have long texts in the desired language! We still have to deal with the diacritics in the preprocessing stage,\n",
    "but this is a good starting point.\n",
    "\n",
    "[Back to top](#title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
